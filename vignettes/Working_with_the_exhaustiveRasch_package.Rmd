---
title: "Working with the exhaustiveRasch package"
subtitle: "for package version 0.2.1"
author: "Christian Grebe and Mirko Schürmann"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Working_with_the_exhaustiveRasch_package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## 1. Introduction

The selection of items from a larger item pool is a challenge in the context of developing rasch valid instruments in practice. For example, items from an item bank can be combined in many different ways to form a scale in order to fulfill the criteria of a rasch scale. A serial and manual procedure to exclude individual items can lead to early exclusion of potentially suitable item combinations. Additionally, in order to derive an appropriate short form from an existing instrument, theoretical considerations are usually required in order to take into account the respective content domains or facets of the long form.

The package *exhaustiveRasch* conducts an exhaustive search over all possible item combinations. It identifies item combinations that meet the item and model fit criteria as defined by the user. Theoretically alowed item combinations can be defined beforehand by pre-defining rules for inclusion and exclusion.

(Semi-) automatic item selection using Rasch principles is also addressed by the *autorasch* package (Wijayanto et al. 2023). In contrast to *exhaustiveRasch,* this package aims to identify exactly one optimal model. ExhaustiveRasch, on the other hand, tests all item combinations (possibly previously reduced on a theoretical basis) against the criteria specified by the user using common model tests for Rasch models. Ultimately, it does not return the one optimal model, but all item combinations that fulfil the specified criteria.

## 2. Package overview

The package consists of two main parts (functions). The first is to define rules for possible item combinations of the scale which should be constructed and it is saved in a list (rules_object). This is used in a second step to calculate all possible item combinations by the use of the funktion apply_combo_rules and is saved in a list object. For Example with the function exhaustice_test all the identified item combinations can be tested to identify all item combinations as candidate models which pass predefined test and criteria for rasch measurement.

### 2.1 Pre-define item combinations: *apply_combo_rules()*

You can use the *apply_combo_rules()* function to define rules for item combinations to be recongnized or permitted in the candidate models. The function needs the *full* argument, a vector of numeric values for the item indices of the full item set to be processed. For example, if the function should be applied to a full set of 10 items, the *full* argument must be set to 1:10.

You can define the length of the scales by setting the *combo_length* argument. This argument can be a single numeric value or a vector of numeric values. For example, with *combo_length=6* only combinations of 6 items are selected. or with *combo_length=4:6* only combinations with at least 4 and not more than 6 items are selected. with *combo_length c(4,7,8)* only combinations with 4,7 and 8 items are selected. If not specified, all scale length between 3 and the maximum number of items in full will be used.

There are four types of rules that can be defined:

-   maximum rule: a maximum of x out of y items

-   minimum rule: at least of x out of y items

-   forbidden rule: item combinations that are not permitted

-   forced: items will be present on any of the selected item combinations.

The way to define maximum, minimum and forbidden rules is to use a list of lists (one list for each rule). For ***minimum and maximum rules*** each list has to contain three values:

-   a character string ("min" or "max") that defines the type of the rule

-   a numeric value that defines the minimum/ maximum value (e.g. 2 for at least/ at most 2 items)

-   a numeric vector with the indices of items to apply the rule to list("min", 1, 1:6) defines a rule for selecting at least one of the items 1-6.

For example, *list("max", 3, 1:6)* defines a rule for selecting at most three of the items 1-6.

A list for a ***forbidden rule*** contains only two values:

the character string "forbidden" that defines the type of the rule

a numeric vector with the indices of items to apply the rule to

list("forbidden", c(8,10)) defines a rule that prevents selecting both of the items 8 and 10 for a candidate model. You have to combine the lists with the minimum, maximum and forbidden rules to one list of lists that contains all the rules to be applied, for example:

```{r Define combination rules}
rules_object <- list()

rules_object[[1]] <- list("min", 1, 1:6)

rules_object[[2]] <- list("max", 3, 1:6)

rules_object[[3]] <- list("forbidden", c(8,10)) 
```

These three rules lead to a selection of candidate models with at least one but at most three of the first six items, while In none of the selected item combinations items 8 and 10 will both be present.

The ***forced rule*** is not defined in that lists of lists. To force items to be selected for any candidate model, use can use the *forced_items* argument of the function. Provide the item indices(s) as a numeric value or a vector of numeric values. forced_items = c(4,7) will ensure that items 4 and 7 will be present in any of the candidate models.

### 2.2 Test model fit: The exhaustive_tests() function

Provide the data to analyze as a data.frame using the *dset* argument.

At first, you have to decide which item combinations for candidate models you want to test. You can choose from three approaches:

-   **Approach A) Test all item combinations with given scale lengths.** Use this approach if you don't have any theoretical considerations in mind that should be addressed by defining rules using the apply_combo_rules function. All item combinations will be tested that meet the number of items provided in the *scale_length* argument. The *scale_length* argument expects a numeric vector, e.g. *c(4:8)* for any item combinations with at least 4 and at most 8 items (see the examples for the *combo_length* argument of the apply_combo_rules function above). If you do not set the *scale_length* argument and do not provide pre-selected item combinations using the *combos* argument (approaches B and C), all possible item combinations will be tested with a minimum scale length of 8 to the maximum scale length (number of items in the data.frame).

-   **Approach B) Use pre-defined item combinations from the result of a previous call of the apply_combo_rules()** function (see above). Use the results object from this call as the *combos* argument.

-   **Approach C) Use results of a previous call of the exhaustive_tests function.** You can use the item combinations that passed a previous call for further tests. This is useful, if the previous call led to a greater number of candidate models that you want to reduce further. For example, you could use tests in the second run that you did not use in the first run. Or you could use stricter criteria in the second run (e.g. use stricter values for the upper and lower bound of itemfit indices, define a stricter level of significance, additionally set criteria for the standardized itemfit indices if you only used MSQ-based indices in the first run or use another split criterion for Anderson's LR Test or other external variables for the DIF-Tree analysis). Use the item combinations from the \$passed_combos list of the results objects of the first run for this approach.

Second, **specify the type of rasch models to fit** using the *modelType* argument. For binary data use "RM" to fit dichotomous rasch models. For polytomous data you can choose between "PCM" for partial credit models or "RSM" for rating-scale rasch models.

Third, **select the tests for model and item fit** you want to use. The tests have to be specified in the *tests* argument as a vector of characters (strings). The tests will be conducted in the order you use in the vector. Table 1 shows the available tests for the *tests* argument. These are described in more detail in the following.

*Table 1: overview of the available tests*

```{r table 1, echo=FALSE}
tab_tests <- c("all_rawscores",
               "no_test",
               "test_DIFtree",
               "test_itemfit",
               "test_LR",
               "test_mloef",
               "test_PSI",
               "test_personsItems",
               "test_respca",
               "test_waldtest",
               "threshold_order"
)

tab_desc <- c("checks, if all possible rawscores (sums of item scores) are empirically respresented in the data",
              "No test is performed, but the returned passed_exRa object contains fit models for the provided item combinations",
              "tests differential item functioning (DIF) related to the specified external variables by using raschtrees; checks, if no split is present is the resulting tree.",
              "checks, if the fit- indices (infit, outfit) are within the specified range",
              "performs Anderson’s likelihood ratio test with the specified split criterion",
              "performs the Martin-Löf test with the specified split criterion",
              "checks if the person separation index (PSI) - also known as person reliabilty exceeds the given value (between 0 and 1).",
              "checks, if there are item thresholds in the extreme low and high range of the latent dimension and/or checks, if the amount of item thresholds between neighboring person parameters is above the specified percentage",
              "performs a principal components analysis on the rasch residuals; checks if the eigenvalue of the highest loading contrast is below the specified value",
              "performs a waldtest with the specified split criterion; checks, if all items have p-values below the specified alpha (or local alpha, if a Bonferroni correction is used)",
              "checks, if all threshold locations are ordered (not applicable for dichotomous rasch models)"
)

tab_param <- c("no arguments",
               "no arguments",
               "no arguments (but DIFvars must be provided)",
               "MSQ in- and outfits between 0.7 and 1.3 and no significant p-values (alpha=0.1, no Bonferroni correction)",
               "median rawscore as split criterion, no significant p-values (alpha=0.1, no Bonferroni correction)",
               "median rawscore as split criterion, no significant p-values (alpha=0.1)",
               "values above 0.8",
               "checks for thresholds in the extreme ranges, but not for the amount of thresholds between person parameters",
               "maximum eigenvalue of 1.5",
               "median rawscore as split criterion, no significant p-values (alpha=0.1, no Bonferroni correction)",
               "no arguments"
)

tab <- as.data.frame(cbind(tab_tests, tab_desc, tab_param))
colnames(tab) <- c("test", "description", "default parametrisation")
knitr::kable(tab) 
```

#### test_itemfit

This tests checks the itemfit indices using the itemfit() function of the eRm package (Mair & Hatzinger 2007). You can define the criteria to use for candidate models to be considered as showing acceptable item fit using the *itemfit_control()* function. This function sets standard values that can be overridden.

-   evaluate only infits (set *outfits* argument FALSE) or infits and outfits (set *outfits* argument TRUE)

-   evaluate only MSQ fits (set *msq* argument TRUE and *zstd* argument FALSE) or only z-standardized fits(set *zstd* argument TRUE and *msq* argument FALSE) or both of them (set both arguments TRUE).

-   evaluate p-values of the chi-squared tests additionally to the fit indices above (set *use.pval* argument TRUE. The level of significance is not to be set in the *itemfit_control()* function but globally in using the *alpha* argument of the *exhaustive_tests()* function). You can also add a Bonferroni adjustment for the p-values (this also has to be set globally for all tests in the *exhaustive tests()* function by setting the *bonf* argument TRUE).

You can either override any of the standard value set by *itemfit_control* with a call to that function (e.g. using *control= itemfit_control(outfits=F, zstd=T). This* will evaluate infits only -- MSQ infits as well as z-standardized infits -- and will use all other arguments with their standard value). Or you can pass *itemfit_control()* arguments directly to the *exhaustive_tests()* function (e.g. use outfits=F as a argument in a call to *exhaustive_tests()*).

In the literature on Rasch analysis, there are many indications on what limits should be applied for the upper and lower limits of the fit indices. The most common reference is Linacre (2002), who gives recommendations for both MSQ fit indices (see table 2) and standardised fit indices (see table 3).

*Table 2: MSQ Infit and outfit values and implications for measurement (Linacre 2002)*

```{r table 2, echo=FALSE}
tab_value <- c("> 2.0",
               "1.5 - 2.0",
               "0.5 - 1.5",
               "< 0.5"
)

tab_implic <- c("Distorts or degrades the measurement system. May be caused by only one or two observations.",
                "Unproductive for construction of measurement, but not degrading.",
                "Productive for measurement.",
                "Less productive for measurement, but not degrading. May produce misleadingly high reliability and separation coefficients."
)
tab <- as.data.frame(cbind(tab_value, tab_implic))
colnames(tab) <- c("MSQ", "implication for measurement")
knitr::kable(tab) 
```

*Table 3: standardised Infit and outfit values and implications for measurement (Linacre 2002)*

```{r table 3, echo=FALSE}
tab_value <- c("≥ 3",
               "2.0 - 2.9",
               "-1.9- 1.9",
               "≤ -2"
)

tab_implic <- c("Data very unexpected if they fit the model (perfectly), so they probably do not. But, with large sample size, substantive misfit may be small.",
                "Data noticeably unpredictable.",
                "Data have reasonable predictability.",
                "Data are too predictable. Other 'dimensions' may be constraining the response patterns."
)
tab <- as.data.frame(cbind(tab_value, tab_implic))
colnames(tab) <- c("standardized value", "implication for measurement")
knitr::kable(tab) 
```

Wright et al. (1996) recommend limits of varying stringency depending on the purpose of the scale being developed (see table 4). In many situations, MSQ fit indices between 0.5 and 1.5 can be considered acceptable, but our default value for test_itemfit is the stricter range between 0.7 and 1.3 for MSQ fit indices or -1.96 - 1.96 for standardised fit indices.

The p-values should be interpreted with caution for large samples, as the hypothesis tests are then typically overpowered.

*Table 4: reasonable MSQ ranges for infit and outfit (Wright et al. 1996)*

```{r table 4, echo=FALSE}
tab_value <- c("MCQ (high stakes)",
               "MCQ (run of the mill)",
               "rating scale (survey)",
               "clinical observation",
               "judged (agreement encouraged)"
)

tab_implic <- c("0.8 - 1.2",
                "0.7 - 1.3",
                "0.6 - 1.4",
                "0.5 - 1.7",
                "0.4 - 1.2"
)
tab <- as.data.frame(cbind(tab_value, tab_implic))
colnames(tab) <- c("type of test", "range")
knitr::kable(tab) 
```

#### test_respca

This test performs a principal components analysis on the rsch residuals ('Rasch PCA') and is a test on the unidimensionality assumption of the rasch model. The criterion to pass this test is the maximum loading for a component (contrast) of this PCA, as defined in the *max_contrast* argument.

#### test_mloef

This test performs Martin-Löf tests using the *MLoef* function of *eRm*. The default split criterion is a split by median. If you want to use another split criterion, you can set this using the *splitcr_mloef* argument. Use "mean" for a split by mean. You also can set a custom split criterion using a numerical vector with two distinct value to define two groups of items (e.g. the even and the odd items). This length of the vector has to match the length of the scale. Therefore, this approach is only feasible if all candidate models have the same number of items (*scale_length* argument). Candidate models pass this test if the null hypothesis is not rejected. The level of significance can be set globally for all tests by using the *alpha* argument.

#### test_LR

This test performs Anderson's likelihood ratio tests using the *LRtest* function of *eRm*. Just like the *test_mloef* function a median split is the default split criterion and a custom split criterion can be used by providing a numerical vector with the argument *splitcr_LR* to define the groups. This vector has to match the number of persons in the dset data.frame. Unlike in the *test_mloef function*, you can define more than two groups as custom split criterion , e.g. you can use "all.R" as a value for *splitcr_LR* to define groups based on the empirical rawscores. You also can use "mean" as a value for *splitcr_LR* to split by mean. See the documentation of the *LRtest* function in *eRm* for more details. Candidate models pass this test if the null hypothesis is not rejected. The level of significance can be set globally for all tests by using the *alpha* argument and a bonferroni correction can be used by setting the *bonf* argument TRUE.

Note that the default split criterion, the median split, is not useful for ordinal models (PCM and RSM), because items are eliminated if they do not have the same number of categories in each subgroup. In exhaustiveRasch, item combinations are considered as not passing test test in this case. The authors of the eRm package suggest to use eigher a random split or a custom (external) split criterion in these cases. We recommend using the LR test for PCM and RSM with an external split criterion (to be passed in the argument *splitcr_LR*), as a random split is not very helpful for the analysis.

#### threshold_order

This test checks if the item threshold locations (beta parameters) of each item are ordered. This is only relevant for polytomous data (*modelType* "PCM" or "RSM") and therefore is meaningless for binary data (*modelType* "RM").

#### test_waldtest

This test performs Wald tests using the *Waldtest* function of *eRm*. The default split criterion is split by median. You can define other split criteria by using the *splitcr_wald* argument, use "mean" to split the individuals by the mean of their raw scores. You can also define a custom split criterion by providing a numeric vector that assigns every person to one of two groups. This vector has to match the number of persons in the dset data.frame. See the documentation of the *Waldtest* function in *eRm* for more details. Candidate models pass this test if the null hypothesis is not rejected. The level of significance can be set globally for all tests by using the *alpha* argument and a bonferroni correction can be used by setting the *bonf* argument TRUE.

Note that the default split criterion, the median split, is not useful for ordinal models (PCM and RSM), because items are eliminated if they do not have the same number of categories in each subgroup. In *exhaustiveRasch*, item combinations are considered as not passing test test in this case. The authors of the eRm package suggest to use eigher a random split or a custom (external) split criterion in these cases. We recommend using the Waldtest for PCM and RSM with an external split criterion (to be passed in the argument *splitcr_wald*), as a random split is not very helpful for the analysis.

Also note that the Waldtest currently leads to different results depending on how the arguments for *estimation_param* (via the *estimation_control()* function) are selected. If *sum0=TRUE* (for *est="psychotools"* this is always the case), then the first beta parameter of the model is 0. This also applies to the submodels of the Wald test, which is why no p-value can be calculated for the first item category (NaN). If this is the only item category with NaN for the p-value (and no p-value is below the local alpha), e*xhaustiveRasch* considers the test to be passed. Behaviour as in eRm requires *est="eRm"* (which will result in longer computation times) and *sum0=TRUE*.

#### test_DIFtree

This test checks for differential item function using the *raschtree* function of the *psychotree* package (the *rstree* or *pctree* function respectively, depending on *modelType*; Strobl et al. 2015, Komboz et al., 2018). You can use several external variables at once that can be binary, as well as categorical or continuous. Provide the external variables as a data.frame using the *DIFvars* argument. The function builds decision trees. Nodes in the tree indicate differential item functioning for the split point that defines the actual tree node. See the documentation on the function rstree, pctree and rstree in pschotree for more details. Candidate models pass this test if the number of tree nodes is 1.

#### test_personsItems

This test analyses the relationship between the person parameter distribution and the item (or: threshold) locations as you would do it when manually inspecting a personitem-map or Wright map (for example when using the plotPImap function of the eRm package. The analysis implemented in this can check two different aspects.

First, you can use the boolean argument extremes (values TRUE or FALSE). This checks if the inspected scale differentiates well in the upper as well as in the lower range of the latent dimension. This is done by checking whether there is an item or threshold location beyond the second highest and second lowest person parameters. Second, you can define the minimum proportion of neighboring person parameters with an item/threshold location in between by using the gap_prop argument. Set this argument to any decimal between 0 and 1 to define the minimum proportion. If set to 0 this check will be ignored. Note that in the case of missing values in your data you will probably will have many different person parameters. In these cases the use of the gap_prop argument is not useful and should be avoided.

#### test_PSI

This test checks whether the person seperation index also known as "person reliability") is at least equal to the selected value. This value must be specified with the *PSI* argument (default: PSI= 0.8)

#### all_rawscores

This test checks if all possible raw scores of the inspected scale are represented in the data. For example, if the scale consists of 4 binary outcomes there are 5 possible raw scores when summing up these items (raw scores 0 to 4). If at least one of these possible raw score does not occur in the data, this test is not passed. Note that the passing or failing of this test does not have any meaning for considering if the scale is rasch valid. But if you have a low number of possible raw scores, you perhaps want to make sure, that these are all represented by the scale. This test is particularly useful for these cases, whereas it is too strict for a larger number of possible raw scores (especially for ordinal items with a high number of response categories) and should be avoided.

#### no_test

This test is not a test in the strict sense. It merely estimates the model parameters and returns a *passed_exRA* object including the @passed_models slot. In the case of dichotomous RM models, however, the remaining item combinations may be reduced if they do not pass the data checks known from the *eRm* package (*"ill conditioned data matrix"*). This "test" is not intended for productive use. However, it can be used to generate a *passed_exRA* object on the basis of which further tests are to be carried out (and with Rasch models already been estimated, which reduces the computation time). The test can also be used to estimate another *passed_exRA* object with modified arguments (with/without standard error, with psychotools/ eRm-based parameter estimation and, in the case of eRm-based estimation, with TRUE or FALSE for the sum0 argument).

### missing data

In the case of missing data, it is possible to ignore cases with missing values in the respective analysis. Set the *na.rm* argument TRUE to remove cases with missing data in each test. These cases are removed in the tests for the respective item combination only, not globally based on the full item set.

### alpha correction

The default alpha value for hypothesis tests is 0.1, because we are interested in not rejecting the null hypothesis in each of the respective tests (itemfit with p-values, waldtest). The alpha value can be defined in the *alpha* argument of the *exhaustive_tests()* function and will be used in all of the specified tests. In tests taht use multiple hypothesis tests (itemfit with p-values, waldtest), you should consider to use an alpha adjustment because of the multiple testing problem. Set the *bonf* argument TRUE to use a Bonferroni correction. The corrected local alpha will then be the criterion for each single p-value within a single test of an item combination. Note, that if you choose for a Bonferroni correction, this will affect all single tests with multiple hypothesis test, it is not possible to use the alpha correction e.g. for the itemfit p-values, but to not use it for the waldtest within the same call to *exhaustive_tests()*. ALso note, that intentionally there is no option for an alpha correction over all tests of a call to *exhaustive_tests()*, bit we consider to add an respective option in a later version of the package.

### other arguments for exhaustive_tests

To help speeding up the analyses, the performed tests are parallelised, which means, that the computations will be split over the cores of your cpu. By default, all of your cpu cores will be used, but you can change that behavior by defining the number of cores to hold out in the ***ignoreCores*** argument. This can be useful if you want to perform a computationally intensive analysis (e.g. polytomous models with a large number of item combinations), but still want to work productively on this machine.

You can customize aspects of parameter estimation using arguments of *estimation_control()* in the ***estimation_param*** argument. You can override the default parameters with a call to this function and providing the argument(s) to override. The *est* argument defines wheter to use the respective functions of the eRm package (value *"eRm"*) or the much faster functions of the package *psychotools* (value *"psychotools"*) for the estimating the item parameters. If "*eRm"* is used, you can also choose, if the item parameters should start with 0 (*sum0=FALSE*) or if they should be summed to be 0 (*sum0=TRUE*). Using *"psychotools"* in the *est* argument will always set *sum0=FALSE*. With the boolean argument *se* you can opt for not calculating standard errors for the item parameters (*se=FALSE*). Note that some tests rely on the standard errors. If they are part of the tests argument, the *se* argument will automatically be set as TRUE. If you provide an object of class *passed_exRa* containing previously fit models that were estimated without standard errors, the models will be re-estimated, if one of the chosen tests relies on standard errors (or on the Hessian matrix, respectively). The arguments est, se and sum0 can also be used directly when calling exhaustive_tests. Arguments not provided will then be set to the default values.

If you do not want to trace the process of the analysis, you can set the ***silent*** argument TRUE to avoid these Outputs to the console.

### 2.3 Results object: the class passed_exRa

The object returned by the *exhaustive_tests* function is an S4 class of the type *passed_exRa*. This class consists of the following slots (because of the S4 class the slot have to be addressed by using \@ rather than \$):

-   ***process***: data.frame with information about the process of the analysis (e.g. number of the passed item combinations after each test) u.a)
-   ***passed_combos***: list of vectors of the passed item combinations
-   ***passed_models***: list of the fit rasch model objects (of the class RM, PCM or RSM) for each of the passed item combinations
-   ***data***: data.frame containing the data used for the analysis
-   ***IC***: information criteria (AIC, BIC, cAIC) for each of the remaining rasch models
-   ***timings***: data.frame containing the runtime of each test

The ***summary*** method for an object of the *passed_exRa* class delivers information about:

1.  the process of the respective call to *exhaustive_tests()*

-   scale lengths that were analysed
-   initial number of item combinations
-   performed tests
-   number of passed item combinations after each test

2.  item importance: absolute and relative frequencies for each item to be used in the passed item combinations
3.  runtime of the analysis

### 2.4 Removing subsets or supersets of other item combinations

Depending of your data and the test criteria used for the *exhaustive_tests()* function, you probably will have a certain number of item combinations left in the *passed_exRa* object, that passed all of your tests. Among these item combinations you will likely have some combinations that represent a subset of another item combination (for example items 1-2-3-4 as well as items 1-2-3-4-6 and 1-2-3-4-9). You can use the *remove_subsets()* function to remove either all subsets of a larger superset or vice versa. This function requires two arguments: Provide your object of class *passed_exRA* in the *obj* argument and set the *keep_longest* argument FALSE (default), if you want to keep the subsets and remove all supersets that contain all items of this subset (principle of economy). If you set *keep_longest* TRUE, the longer superset will be kept and all subsets consisting of a subset of this superset will be removed (principle of maximizing information).

## 3. Datasets

Currently, the package comes with three datasets:

**ADL:** dichotomous data for activities of daily living

**InterProfessionalCollaboration:** polytomous data with four item categories for interprofessional collaboration from nurses, midwifes, occupational therapists, physiotherapists ans speech therapists, measures with the Health Professionals Competence Scales (Grebe et al. 2021).

**cognition:** polytomous data with five item categories for perceived cognitive functioning, measures with the FACT-cog (Cella 2017).

All of these datasets come with socio-demographic overhead variables that can be used for analyses of differential item functioning. See the package documentation for item labels and answer categories.

## 4. Example: Activities of daily living (binary data)

Activities of daily living (ADL) is a concept used in geriatrics, gerontology, nursing and other health-care related professions that refers to clients' routine self-care activities. ADL measures are widely used as measures of functioning in different healthcare settings. ADLs are key components in healthcare payment systems in most countries. The concept was first developed by Katz (Katz et al. 1963). This ADL measure used six activities: bathing, dressing, toileting, transferring, bladder and bowel continence and eating. The widely used ADL index of the Ressource Utilization Groups comprises

There is good empirical evidence that the various ADL activities are typically maintained for different lengths of time as the need for care progresses. Dressing, personal hygiene and toilet use can be considered as "early loss" ADLs. Transfer, locomotion and bed mobility are "middle loss" ADLs, while the ability to eat independently generally remains the longest (Morris et al. 1999).

In the ADL data that comes with the package (Grebe et al. 2013) there are 15 ADL items. The first six items address aspects of mobility (transferring, standing, walking and bed mobility). The next three items address personal hygiene (including taking a shower). There are two items for dressing, two items for eating/drinking and one item for toileting. We can subsume the last item (intimate hygiene) to toileting or to personal hygiene respectively. Let us assume that we want to construct an ADL index that preferably consists of at least one item for mobility, personal hygiene/dressing, eating/drinking and toileting. At least we do not want to overrepresented items that address the same activity. So, we are only interested in scales that use at least one but not more than two items for each activity. We consider scales with at least four items and with a maximum of eight items. Additionally, we do not want to have both of the first two items in the scale as both of them address transferring. We can set up these combination rules as follows:

```{r Define ombination rules (ADL example)}
library(exhaustiveRasch)
data(ADL)
rules_object <- list()
rules_object[[1]] <- list("max", 2, 1:6) #mobility
rules_object[[2]] <- list("min", 1, 1:6) #mobility
rules_object[[3]] <- list("max", 2, 7:11) # personal hygiene/dressing
rules_object[[4]] <- list("min", 1, 7:11) # personal hygiene/dressing
rules_object[[5]] <- list("min", 1, 12:13) # eating/drinking
rules_object[[6]] <- list("min", 1, 14:15) # toileting
rules_object[[7]] <- list("forbidden", c(1,2)) # transfer from bed/ stand up from chair
```

The *apply_combo_rules()* function provides all item combinations that match our pre-defined rules. We use our rules_object als the *rules* argument and define the permitted scale lengths in the *combo_length* argument:

```{r Apply combination rules (ADL example)}
final_combos <- apply_combo_rules(combo_length= 4:10, full=1:15, rules= rules_object)
```

Without applying any rules, there are 22.243 combination of 15 items with scales lengths between four and eight (which is the sum of the binomial coefficients of these scale lengths). Our applied rules reduce the number of permitted item combinations to 2.700 based on theoretical presumptions. These item combinations can now be used in a rasch analysis. The threshold_order function is not necessary in this example, because the data is binary. We want to use the Martin-Löf-Test and Anderson's likelihood-ratio test, both with the median rawscore as split criterion. For itemfit we are fine with MSQ-in- and outfits between 0.5 and 1.5. We do not mind neither the standardised fit indices nor the p-values of the chi-squared tests for item fit. For the likelihood-ratio test, the Martin Löf test and the Waldtest we use a significance level of p=0.1, as we are interested in confirming the null-hypothesis and want to reduce type-1 errors. But we want to address the multiple comparisons problem (alpha inflation) at least at the level of each test and use a Bonferroni correction. For these assumptions we can use the standard arguments, but we have to overrun the default value for the *bonf* argument, as well as the values for MSQ itemfit. We could do that by overrunning the respective arguments of the *itemfit_control()* function, but we also can pass these arguments directly to *exhaustive_tests()*, the main function of the the package. In the *tests* argument we have to specify all test functions we want to use. These tests will then be executed in the order specify. There is no need for the argument *scale_length*, as we have already pre-defined the item combinations to use. We pass our rules_object to the function instead, using the *combos* argument. So our call to the exhaustive tests function is:

```{r Run tests (ADL example)}
passed_ADL <- exhaustive_tests(dset=ADL, combos=final_combos, modelType= "RM",
                               upperMSQ=1.5, lowerMSQ=0.5, use.pval=F, bonf=T,
                               na.rm=T, tests=c("test_itemfit",
                                                "test_mloef",
                                                "test_LR"))
```

After this step of the analysis, only 9 item combinations remain that meet the criteria applied (28 fulfilled the criteria applied for the item fit; of these, 13 remained after the Martin-Löf test and of those again 9 after the likelihood ratio test). From the item importance section of the summary we learn, that two items (eating and intimate hygiene) are part of all the remaining item combinations, while three items (walking, standing and toilet use) each are only represented in one of the item combinations.

We would now like to examine the remaining 9 item combinations with regard to differential item functioning (DIF). For this purpose, we use the variables sex and age, that are available in the ADL data set. We could pass the remaining item combinations to the exhaustive_tests function in the *combos* argument, as we have previously passed the item combinations resulting from the call to the *apply_combo_rules()* function. However, the *combos* argument also accepts the entire S4 class returned by our first call to *exhaustive_tests()*, which we received as *passed_ADL*. Using the entire S4 class has the advantage that the fit models are also passed and do not have to be estimated again.

```{r Run additional test (ADL example)}
passed_ADL2 <- exhaustive_tests(
  dset=ADL, combos=passed_ADL, DIFvars=ADL[16:17], tests=c("test_DIFtree"))

```

All 9 item combinations pass this step of our analysis. But among these item combinations are combinations that represent a subset of another item combination. In the sense of the principle of economy, we look for the shortest scale in each case. To do this, we can use the function *remove_subsets()* to remove the supersets.

```{r Remove subsets (ADL example)}
passed_rem <- remove_subsets(passed_ADL2, keep_longest=F)
```

This procedure removes two item combinations, leaving 7.

## 5. Computation time and considerations for the sequence of tests

Estimating the person parameters of Rasch models is computationally expensive, especially with an increasing number of model parameters. Despite the distribution of the calculations to (up to) all available CPU cores due to the parallelization used in the exhaustiveRasch package, the execution of the model tests including the verification of the applied criteria can require long computing times. This is particularly important for PCM models with many model parameters (number of items and response categories).

The following therefore applies: Many cores (and a processor generation that is as up-to-date as possible) help a lot. Not only a higher number of physical cores is useful here, a higher number of virtual cores (threads) is also helpful. For analyses with a high number of item combinations (\>20 items) and more than four response categories, a calculation in a cloud computing environment may be useful.

Since version 0.21, in addition to the CML estimation algorithms from the eRm package, the use of CML estimation algorithms from the psychotools package is also available (default from version 0.21). This reduces the calculation time for estimating the item parameters by a factor of 3 to 8, depending on the scenario.

However, even when using psychotools-based parameter estimation, some consideration should be given to a productive sequence of tests to be used before carrying out the analyses. The tests with the shortest runtime are those based exclusively on the item parameters and which do not require any estimation of the person parameters: test_mloef, test_waldtest and test_LR. However, the expected trade-off between runtime and reduction of the remaining item combinations after the test should be considered less than the pure runtime of a test. For example, the fastest test is usually the Martin-Löf test (test-mloef). Depending on the characteristics of the selected item combinations and depending on the choice of alpha level (argument alpha) and the use of a Bonferroni correction (argument bonf), however, the Wald test (test_waldtest) may select the remaining item combinations much stricter.

A test of the fit indices (test_itemfit) is probably desirable in every scenario. However, this test has a comparatively long runtime and should therefore be carried out at a later point in time when the number of remaining item combinations has already been significantly reduced.

Testing for differential item functioning using test_DIFtree can require a comparatively long runtime for polytomous items (PCM or RSM models). This also increases with the number of external variables to be tested (argument DIFvars) and if these are metrically scaled variables or categorical variables with many factors.

Not to compute standard errors reduces the calculation time for parameter estimation, as the Hessian matrix is not estimated. However, tests that require the threshold parameters (currently: threshold_order, test_personsItems) rely on the Hessian values being available. If several tests are specified in the tests argument of exhaustive_tests, the hessians are generally estimated if one of the tests requires them. If a passed_exRA object whose Rasch models were estimated without hessians is passed in the combos argument, the item parameters (including hessians) are re-estimated if one of the above-mentioned tests was specified in the tests argument.

In general, the individual tests for analyses with longer runtimes (PCM models with \>4 response categories and several 1,000 item combinations) should not be carried out in a single call of exhaustive_tests. If no item combinations remain after a test, there is no longer access to the item combinations that remained after the penultimate test. A workflow would be better here in which the first test is carried out first (e.g. argument tests=test_mloef). Then the passed_exRA object is passed in the combos argument for the second test. See the example with passed_ADL and passed_ADL2 in section 3 of this vignette.

## References

***Grebe C (2013).*** "Pflegeaufwand und Personalbemessung in der stationären Langzeitpflege. Entwicklung eines empirischen Fallgruppensystems auf der Basis von Bewohnercharakteristika". Oral presentation at the 3-Länderkonferenz Pflege & Pflegewissenschaft, September 2013, Konstanz.

***Katz S, Ford AB, Moskowitz RW, Jackson BA, Jaffe MW (1963).*** "Studies of illness in the aged: the index of ADL: a standardized measure of biological and psychosocial function". jama, 185(12), 914-919. <doi:10.1001/jama.1963.03060120024016>.

***Komboz B, Zeileis A, Strobl C (2018).*** "Tree-Based Global Model Tests for Polytomous Rasch Models." Educational and Psychological Measurement, 78(1), 128--166. <doi:10.1177/0013164416664394>.

***Linacre JM (2002).*** "What do infit and outfit, mean-square and standardized mean?" Rasch Measurement Transactions, 16 (2), 878.

***Mahoney FI, Barthel DW (1965).*** "Functional evaluation: the Barthel index". Maryland state medical journal, 14(2), 61-65.

***Mair P, Hatzinger R (2007).*** "Extended Rasch modeling: The eRm package for the application of IRT models in R." Journal of Statistical Software, 20. doi: 10.18637/jss.v020.i09.

***Morris JN, Fries BE, Morris SA (1999).*** "Scaling ADLs within the MDS". The Journals of Gerontology: Series A, 54(11), M546-M553. <doi:10.1093/gerona/54.11.m546>

***Strobl C, Kopf J, Zeileis A (2015).*** "Rasch Trees: A New Method for Detecting Differential Item Functioning in the Rasch Model." Psychometrika, 80(2), 289--316. <doi:10.1007/s11336-013-9388-3>.

**Wijayanto F, Bucur IG, Groot P, Heskes T (2023)**. autoRasch: An R Package to Do Semi-Automated Rasch Analysis. *Applied Psychological Measurement*, *47*(1), 83-85.

***Wright BD, Linacre JM, Gustafson JE, Martin-Löf P (1996)*** "Reasonable mean-square fit values". Rasch measurement transactions, 2, 370.

***Zeileis A, Strobl C, Wickelmaier F, Komboz B, Kopf J, Schneider L, Debelak R (2023)***. "psychotools: Infrastructure for Psychometric Modeling". R package version 0.7-3, [https://CRAN.R-project.org/package=psychotools](https://cran.r-project.org/package=psychotools).
